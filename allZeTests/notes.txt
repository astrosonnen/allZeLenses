2017-6-30

Added a powerlaw model.
Idea: simulating Ddt measurements by fitting of powerlaw density profiles to doubles. Doing hierarchical inference of H0 and distribution of power-law slopes.
Image positions are known exactly. The only free parameter in the lens model is the density slope, which determines everything else, including the time-delay distance.
Assuming we can measure gamma with a 0.04 precision.

Hierarchical inference does not help in this case, because the intrinsic scatter of the gamma distribution is larger than the precision in the individual measurements.

Problem: the inference on H0 and on the mean gamma appear to be slightly biased.

2017-7-7

Generated mock in which I set a minimum value for the magnification of the inner image (mock A).
Hierarchical inference recovers truth in most parameters, except sigma_*, beta_*.

Generaed mock in which I set a maximum value for the source position, equal to half of the radial caustic (to avoid systems that are too asymmetric, mock C).
Hierarchical inference, done with individual chains with source position allowed to reach the caustic, fails.
Do we need to model the source position distribution as well?
Using chains with the source that goes only go up to half of the caustic produces a nice posterior (if allowed to run long enough. 20,000 points is too short).


2017-7-8

Source position distribution modeling.
Problem: in general, the distribution of source position will not be uniform, due to selection effects (for ex. lenses with too asymmetric configuration cannot be used).
Is it possible to have hyper-parameters for the source position distribution and try to recover it in the most general case?

Experiment: mockD
- generate power-law lenses and image position observations (remove one degree of freedom to make things easier)
- only keep lenses that do not exceed a maximum asymmetry in the configuration
- Look at the distribution in source position (in units of what? caustic? What about lenses over-isothermal that have no radial caustic?)
- Sample each lens, using a fixed bound for the source position (maybe up to the image position?).
- Do hierarchical inference, with or without modeling the source position distribution (and what about the Einstein radius distribution?)

Note: maybe I should just add a small core radius to the power-law profile, so that it always has a radial caustic.

2017-7-10

Experiment: mockE

cored power-law profile.
Fitted using only image positions (and time-delays).
The sample mockE has been generated by throwing away lenses with image configuration more asymmetric than a threshold. As a result the source position distribution is no longer uniform within the radial caustic.
The inference is biased, both if I use a fixed bound for the source position parameter and if I set it to the caustic.

I tried to model the source position distribution.
I describe the distribution in s^2/c^2 (s=source position, c=radial caustic) as pi/2 + arctan between 0 and 1.
If I use the chains obtained with a fixed upper bound on the source position, the inference fails, unless I multiply the likelihood by 1/c^2.
By using a fixed bound, we are favoring lens models with a larger lensing cross-section.

In any case, the final inference on mockE is a non-inference because there is a huge degeneracy between all model parameters, probably due to the fact that I'm using image positions alone to constrain a power-law model.

2017-7-14

From the tests carried out so far, it appears to be critically important to
1- Have an interim prior that constrains the source position to within the radial caustic (using the information that the number of observed images is two)
2- Model the source position distribution with hyper-parameters

However, depending on the choice of the model density profile, the radial caustic is not well defined. Especially for power-law lenses with slope steeper than isothermal: the definition of radial caustic is somewhat arbitrary.
What to do?

2017-8-11

I will try fitting the deV + NFW "simple reality" model with broken power-law models, then attempt a hierarchical model and see if I can still recover H0.
I can.

At the same time, if I do the "dumb inference", marginalizing over all parameters and looking only at the time delay distances, I get a biased H0.


2017-8-13

Fitting a broken power-law hierarchical model to mockC recovers H0 to 1-sigma, with a 5% precision.
That's good, but there are some questions we need to answer.

- How can I bring precision down to 1%? Is increasing precision on image position (currently at 0.1 arcsec) sufficient? How about improving precision on time delays (currently at 5 days)?
- Is the model still accurate when the precision on H0 is 1%?
- If I fit a simple power-law model but I parametrize the source position in the same way I am doing it with the broken power-law, do I still get a wrong H0 inference? (In other words, do we really need to use broken power-law models?)
- Is the dependence on Rein/Reff I have currently in the broken power-law model really necessary? (To be answered after precision on H0 is increased to 1%)

Let's make a new mock, this time using a maximum value for the image asymmetry.
From my Taylor expansion argument, a broken power-law model can provide a 1% accuracy description of the time-delay distance if the image position displacement is within 20% of the Einstein radius. That's a 0.2 limit on the asymmetry parameter, defined as
Chi = (theta1 + theta2)/(theta1 - theta2), with theta2 < 0.

this is mockF, generated by running script 'make_simple_reality_maxasymm_mock.py'.

I've found that:
- The precision on H0 when fitted with power-law profiles is around 1%. The reason why the precision didn't increase with respect to previous mocks, despite the big improvement in image position and, more importantly, time delay precision, is because now lenses have much more symmetric configurations.
- Dumb inference and hierarchical inference with power-law models give the same, wrong, answer.
- Hierarchical inference with broken power-law model also give the wrong answer.


2017-8-14

Today's question is: if I could magically measure the first three derivatives of the lens potential directly, would I be able to make an accurate inference with broken power-law models?

I fitted broken power-law models to image positions (with 0.01 arcsec precision), psi2 and psi3 (with 0.01 precision each) for mockF.
Individual chains look fine. However, joint "dumb" inference on H0 gives slightly biased result: 71.3 +/- 0.5. It's a 2% error, instead of the 1% expected from the Taylor expansion argument. Could it have to do with the priors? I doubt, because the individual chains are very much constrained, it's hard to think that hierarchical inference would help.

Then I looked at the distribution in psi2, psi3 as a function of galaxy properties: stellar mass, size and rein/reff. I don't see any correlation.

Question 2: how are model lenses distributed in the gamma-beta space, for my broken power-law models fitted directly to the potential?
Answer: they are distributed along the line beta = gamma - 2, or in other words, gamma - beta = constant, as expected.

Question 3: could it be that my hierarchical model gets the source position distribution wrong?
Let's try to fit my model to the true source position distribution, and see if it's a good description.
Looks reasonable to me...

It turns out, if I take the individual broken power-law chains, obtained with flat priors on gamma and beta, then calculate H0 for each lens, assuming I know the time-delay exactly, then combine the inferences on H0 (where the only uncertainty is that on the lens model), I get the right answer to 1%.
How come if I do the stupid thing I get the right answer, while if I do the fancy hiearchical inference I get the wrong answer?
It must have to do with the uncertainty on the time-delay.
If I re-run the hierarchical inference with 0.1 day uncerainty (instead of 1 day), I get a posterior that is consistent with H0, though still with a huge degeneracy with the mean slope (in fact the bulk of the posterior is away from 70).

Why does the uncertainty on the time-delay has such a large impact, and why does it bias the inference?

Hold on.

Part of the mess is created by the uncertainty on the time delay.
If I fit mockF with a broken power-law model, and I reduce the uncertainty on dt to 0.03 days, I recover the right answer.
However, the same thing does not work with mockG (though I couldn't use 0.03 as dt, because 'logp value falls outside support' crap. I used dt=0.1 day instead).
There's more.
If I fit a simple power-law model to mockF, with the uncertainty on dt set to 0.03 days, then I also get the right answer back!
I can't quite try the same thing on mockG because it cannot do importance sampling with such a small error.

So, maybe the issue is importance sampling?
Shall I try with Gibbs sampling? The power-law case should be feasible.

Trying with Gibbs sampling. There seems to be a bug in the calculation of the model time delay, as the inferred H0 is too low.

Also, I've realized that mockF (and I guess all the other mocks) have too small image separations, probably as a result of the low stellar mass.


2017-8-15

Gibb sampling works! It works when trying to infer the slope distribution for a set of power-law lenses with image position and radial magnification ratio measurements.
When fitting power-law models to 30 lenses of mockF, I get an accurate inference of H0: H0 = 71.9 +/- 2.4, although the chain (1000 samples) doesn't look too healthy.
mockG gives a slightly biased answer, but much more precise: H0 = 71.2 +/- 0.6.

Now I want to make a new sample similar to mockF, but with larger stellar mass, so that the time delays go up and, hopefully, the precision improves too.
I'm also going to run Gibbs sampling on a 100 lens sample.

Why is time delay so insensitive to beta and gamma, sometimes? I guess that's because it's mostly the source position that matters?

2017-8-16

Gibbs sampling works only for power-law models.
When used to fit broken power-law models, it drifts forever without converging to a stable solution. Even when reality is a broken power-law.

I tried importance sampling again, with mockJ. Still not working. Curiously, the inferred value of H0 is exactly at the middle of the prior on H0, 75. What happens if I change the lower bound from 50 to 40? Nothing, it stays at 75.

The dumb inference shows something funny. The product of the histograms in H0 looks very unstable, especially for the last 20 samples. But that's an effect of finite bin size, which might be one more reason to move to characterizing P(H_0|d) on a grid, instead of working with samples.
But to do that I have to calculate the evidence, i.e. the integral of the likelihood marginalized over the other (4) lens parameters.


