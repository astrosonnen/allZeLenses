{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failed Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shit goes wrong.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Uber random interim chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling procedure that we use is the following: we first sample the interim posterior of individual object parameters, then sample the hyperparameters using those interim chains to marginalize over the individual parameters via Monte Carlo integration and importance sampling.\n",
    "We need to use long chains when sampling the individual galaxies because we want to make sure those chains are converged, but then we don't really need a whole lot of points to do the marginalization, so we just pick a subset of samples.\n",
    "One might be tempted to extract a subsample by choosing points at random along the chain, using the numpy command 'numpy.random.choice'. If you do that, you can get biased results. Better to just thin the chain by, say, picking one every ten points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prior on source position ignores the size of the caustic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is kind of a weird issue that I would like to test again. When sampling the individual lens we need to specify a prior for the source position. I used to just take a uniform prior on the position within a circle of a sufficiently large radius. I got a biased inference on the hyperparameters. Then I started using a dynamic prior: at each step of the chain, for a given set of lens structural parameters (stellar mass, halo mass..) I calculate the position of the radial caustic and I use that as an upper bound on the source position. The bias goes away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interim chains are too short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a 15000 sample chain for a 4 parameter model and a reasonably smooth posterior would seem like a reasonably conservative choice. It turns out if you run these chains a factor of 10 longer, even though they don't look any different from those shorter ones, you can get very different results when you do the full hierarchical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
